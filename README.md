# Collective Communication Operations on Mesh Topologies

**Operations:** Broadcast (Bcast) and Gather  
**Topologies:** 2D and 3D Mesh  
**Algorithms:** Dimension-Ordered Routing (DOR) and Flooding (BFS)  
**Authors:** Aniket Gupta (2022101099) & Samarth Srikar (2022101106)

---

## ğŸ“‹ Table of Contents

1. [Project Overview](#project-overview)
2. [Features](#features)
3. [File Structure](#file-structure)
4. [Installation](#installation)
5. [Usage](#usage)
6. [Algorithm Design](#algorithm-design)
7. [Implementation Details](#implementation-details)
8. [Evaluation Methodology](#evaluation-methodology)
9. [Experimental Results](#experimental-results)
10. [Best Configuration](#best-configuration)
11. [Visualizations](#visualizations)
12. [References](#references)

---

## ğŸ¯ Project Overview

This project implements and analyzes collective communication operations on mesh-based networks. We compare:

- **Topologies:** 2D Mesh vs 3D Mesh
- **Algorithms:** Dimension-Ordered Routing (DOR) vs Flooding (BFS)
- **Operations:** Broadcast and Gather

The goal is to determine the optimal configuration for minimizing latency (sequential hops) and bandwidth usage (message count) in distributed systems.

---

## âœ¨ Features

- âœ… **Mesh Topology Creation** - 2D and 3D mesh with automatic dimension calculation
- âœ… **Broadcast Operation** - Root disseminates data to all nodes
- âœ… **Gather Operation** - All nodes send data to root
- âœ… **DOR Algorithm** - Dimension-ordered routing for optimal message count
- âœ… **Flooding Algorithm** - BFS-based routing for fault tolerance
- âœ… **Performance Analysis** - Latency-bandwidth model simulation
- âœ… **Comparative Visualization** - Comprehensive plots comparing all configurations
- âœ… **Simulation Mode** - Run experiments without MPI hardware

---

## ğŸ“ File Structure

```
Project/
â”œâ”€â”€ mesh_topology.py         # 2D and 3D mesh topology implementation
â”œâ”€â”€ broadcast.py             # Broadcast operations (DOR)
â”œâ”€â”€ gather.py                # Gather operations (DOR)
â”œâ”€â”€ flooding.py              # Flooding algorithm implementation
â”œâ”€â”€ ablation_study.py        # DOR vs Flooding comparison with MPI
â”œâ”€â”€ simulation_study.py      # Simulation-based experiments (no MPI required)
â”œâ”€â”€ main.py                  # Main driver program
â”œâ”€â”€ performance_analysis.py  # Performance measurement utilities
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md                # This file
â”œâ”€â”€ ANALYSIS.md              # Detailed experimental analysis
â””â”€â”€ results/                 # Generated plots and reports
    â”œâ”€â”€ simulation_comparison.png
    â”œâ”€â”€ message_complexity.png
    â”œâ”€â”€ steps_2d_vs_3d.png
    â”œâ”€â”€ combined_analysis.png
    â””â”€â”€ ...
```

---

## ğŸ”§ Installation

### Prerequisites

- Python 3.7+
- MPI implementation (OpenMPI or MPICH) - optional for simulation mode

### Install MPI (Optional - for real MPI experiments)

```bash
# Ubuntu/Debian
sudo apt-get install openmpi-bin libopenmpi-dev

# macOS
brew install open-mpi
```

### Install Python Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸš€ Usage

### Simulation Mode (No MPI Required)

Run comprehensive simulation comparing all configurations:

```bash
python simulation_study.py
```

This generates:
- Detailed console output with results
- Visualization plots in `results/` directory

### MPI Mode

Run with actual MPI processes:

```bash
# 16 processes (4x4 2D mesh)
mpiexec -n 16 python main.py

# 27 processes (3x3x3 3D cube)
mpiexec -n 27 python main.py

# Ablation study: DOR vs Flooding
mpiexec -n 16 python ablation_study.py
```

### Individual Components

```bash
# Test Broadcast only
mpiexec -n 16 python broadcast.py

# Test Gather only
mpiexec -n 16 python gather.py

# Test Flooding
mpiexec -n 16 python flooding.py
```

---

## ğŸ“ Algorithm Design

### Mesh Topology Construction

#### 2D Mesh

```
Coordinate Mapping:
  rank â†’ (row, col)
  row = rank // cols
  col = rank % cols

Neighbors: North, South, East, West (4 neighbors max)

Example 4Ã—4 Mesh:
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ 0 â”‚ 1 â”‚ 2 â”‚ 3 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚ 4 â”‚ 5 â”‚ 6 â”‚ 7 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚ 8 â”‚ 9 â”‚10 â”‚11 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚12 â”‚13 â”‚14 â”‚15 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
```

#### 3D Mesh

```
Coordinate Mapping:
  rank â†’ (x, y, z)
  z = rank // (x_dim Ã— y_dim)
  y = (rank % (x_dim Ã— y_dim)) // x_dim
  x = rank % x_dim

Neighbors: Â±X, Â±Y, Â±Z directions (6 neighbors max)
```

### DOR (Dimension-Ordered Routing)

Routes messages by traversing one dimension at a time in a fixed order.

#### 2D DOR Broadcast Algorithm

```
Phase 1: Row Broadcast
  â””â”€ Root broadcasts along its row
  â””â”€ Sequential hops: (cols - 1)

Phase 2: Column Broadcast (Parallel)
  â””â”€ Each row node broadcasts down its column
  â””â”€ Sequential hops: (rows - 1)

Total Hops: (rows - 1) + (cols - 1)
Total Messages: (cols - 1) + cols Ã— (rows - 1) = p - 1
```

**Visual Example (4Ã—4 mesh):**
```
Phase 1:    0 â†’ 1 â†’ 2 â†’ 3     (3 hops)
Phase 2:    â†“   â†“   â†“   â†“     (3 hops)
            4   5   6   7
            â†“   â†“   â†“   â†“
            8   9  10  11
            â†“   â†“   â†“   â†“
           12  13  14  15
           
Total: 6 hops
```

#### 3D DOR Broadcast Algorithm

```
Phase 1: X-axis Broadcast     â†’ (x_dim - 1) hops
Phase 2: Y-axis Broadcast     â†’ (y_dim - 1) hops
Phase 3: Z-axis Broadcast     â†’ (z_dim - 1) hops

Total Hops: (x - 1) + (y - 1) + (z - 1)
Total Messages: p - 1
```

### Flooding (BFS) Algorithm

Each node forwards data to ALL its neighbors.

```
Level 0: Root has data
Level 1: Root â†’ all neighbors
Level 2: Level-1 nodes â†’ all their neighbors (except sender)
...
Level k: All nodes at Manhattan distance k receive data

Sequential Hops: Maximum Manhattan distance from root
Total Messages: Number of edges in mesh
                (each node forwards to all neighbors except the one it received from)
```

**Key Difference from DOR:**

| Aspect | DOR | Flooding |
|--------|-----|----------|
| Message Count | p - 1 | Number of edges |
| Redundancy | None | Minimal |
| Fault Tolerance | Low | High |

---

## ğŸ“Š Implementation Details

### Mesh2D Class

```python
class Mesh2D(MeshTopology):
    def __init__(self, comm):
        self.grid_size = int(math.sqrt(size))
        self.rows = self.cols = self.grid_size
        self.coords = self._rank_to_coords(rank)
        self._calculate_neighbors()
    
    def _rank_to_coords(self, rank):
        return (rank // self.cols, rank % self.cols)
    
    def _calculate_neighbors(self):
        # Add north, south, east, west neighbors
```

### Mesh3D Class

```python
class Mesh3D(MeshTopology):
    def __init__(self, comm):
        self.grid_size = int(round(size ** (1/3)))
        self.x_dim = self.y_dim = self.z_dim = self.grid_size
        self.coords = self._rank_to_coords(rank)
        self._calculate_neighbors()
    
    def _rank_to_coords(self, rank):
        z = rank // (x_dim * y_dim)
        y = (rank % (x_dim * y_dim)) // x_dim
        x = rank % x_dim
        return (x, y, z)
```

### Broadcast Implementation

```python
def broadcast_2d_mesh(mesh, data, root=0):
    # Phase 1: Row broadcast using MPI Split
    row_comm = comm.Split(color=row, key=col)
    data = row_comm.bcast(data, root=root_col)
    
    # Phase 2: Column broadcast
    col_comm = comm.Split(color=col, key=row)
    data = col_comm.bcast(data, root=root_row)
    
    # Calculate sequential hops
    communication_steps = (cols - 1) + (rows - 1)
    return data, time, steps, messages
```

### Flooding Implementation

```python
def broadcast_flooding(mesh, data, root=0):
    # BFS level-by-level propagation
    for level in range(max_distance):
        if my_distance == level and have_data:
            # Send to ALL neighbors
            for neighbor in get_neighbors(mesh, rank):
                comm.send(data, dest=neighbor)
                msgs_sent += 1
        
        elif my_distance == level + 1:
            # Receive from any neighbor
            data = comm.recv(source=MPI.ANY_SOURCE)
    
    return data, time, max_hops, msgs_sent
```

---

## ğŸ“ˆ Evaluation Methodology

### Metrics

1. **Sequential Hops (Latency)**
   - Number of sequential message-passing steps
   - Lower is better

2. **Message Complexity (Bandwidth)**
   - Total messages transmitted
   - Lower is better

3. **Simulated Time**
   - Based on latency-bandwidth model: `T = hops Ã— (ts + tw Ã— m)`
   - ts = 10 Î¼s (startup latency)
   - tw = 10 ns/byte (bandwidth time)
   - m = 8000 bytes (message size)

### Comparison Approach

- **Fair Comparison:** Match node counts between 2D and 3D
- **Configurations tested:**

| Nodes | 2D Mesh | 3D Mesh |
|-------|---------|---------|
| 8 | 2Ã—4 | 2Ã—2Ã—2 |
| 16 | 4Ã—4 | 2Ã—2Ã—4 |
| 64 | 8Ã—8 | 4Ã—4Ã—4 |
| 256 | 16Ã—16 | 4Ã—8Ã—8 |

---

## ğŸ“Š Experimental Results

### Sequential Hops Comparison

| Nodes | 2D DOR | 3D DOR | Improvement |
|-------|--------|--------|-------------|
| 8 | 4 | 3 | **25.0%** |
| 16 | 6 | 5 | **16.7%** |
| 64 | 14 | 9 | **35.7%** |
| 256 | 30 | 17 | **43.3%** |

### Message Complexity Comparison

| Nodes | DOR Messages | Flooding Messages | Ratio |
|-------|--------------|-------------------|-------|
| 16 | 15 | 24 | 1.6Ã— |
| 64 | 63 | 112 | 1.8Ã— |
| 256 | 255 | 480 | 1.9Ã— |

### Key Findings

1. **3D mesh reduces latency by up to 43%** compared to 2D for large networks
2. **DOR uses 1.5-2Ã— fewer messages** than flooding
3. **Improvement grows with network size** due to O(âˆ›p) vs O(âˆšp) scaling

---

## ğŸ† Best Configuration

### Winner: **3D Mesh + DOR Algorithm**

| Criterion | Performance | Score |
|-----------|-------------|-------|
| Latency | Minimum (3(âˆ›p - 1) hops) | â­â­â­â­â­ |
| Bandwidth | Optimal (p - 1 messages) | â­â­â­â­â­ |
| Scalability | Best (O(âˆ›p) growth) | â­â­â­â­â­ |
| Predictability | Deterministic | â­â­â­â­â­ |
| Implementation | Moderate complexity | â­â­â­â­ |

### Why 3D + DOR is Optimal

1. **Latency Advantage:**
   - 3D mesh has smaller diameter: 3(âˆ›p - 1) vs 2(âˆšp - 1)
   - For 256 nodes: 17 hops vs 30 hops (43% reduction)

2. **Bandwidth Efficiency:**
   - DOR sends exactly p - 1 messages (minimum possible)
   - Flooding wastes bandwidth with redundant messages

3. **Scalability:**
   - âˆ›p grows slower than âˆšp
   - Advantage increases with network size

4. **Practical Considerations:**
   - Deterministic routing â†’ predictable performance
   - No message duplication â†’ reduced network congestion

### Recommendation by Use Case

| Use Case | Recommendation |
|----------|----------------|
| HPC Clusters | 3D + DOR |
| Fault-Tolerant Systems | 3D + Flooding |
| Simple Deployments | 2D + DOR |
| Small Networks (<8 nodes) | 2D + DOR |

---

## ğŸ“‰ Visualizations

Generated plots in `results/` directory:

| File | Description |
|------|-------------|
| `simulation_comparison.png` | 4-panel comparison of all configurations |
| `message_complexity.png` | DOR vs Flooding message count |
| `steps_2d_vs_3d.png` | Sequential hops with improvement % |
| `combined_analysis.png` | Comprehensive 4-panel analysis |
| `scalability_analysis.png` | Theoretical scaling curves |
| `time_comparison.png` | Simulated time vs nodes |
| `comparable_configs.png` | Direct 2D vs 3D comparison |

---

## ğŸ”¬ Theoretical Background

### Latency-Bandwidth Model

```
T_msg = ts + tw Ã— m

Where:
  ts = startup latency (time to initiate communication)
  tw = time per word (inverse of bandwidth)
  m  = message size
```

### Complexity Analysis

| Topology | Algorithm | Sequential Hops | Messages |
|----------|-----------|-----------------|----------|
| 2D Mesh | DOR | 2(âˆšp - 1) | p - 1 |
| 2D Mesh | Flooding | 2(âˆšp - 1) | rowsÃ—(cols-1) + colsÃ—(rows-1) |
| 3D Mesh | DOR | 3(âˆ›p - 1) | p - 1 |
| 3D Mesh | Flooding | 3(âˆ›p - 1) | (x-1)yz + x(y-1)z + xy(z-1) |

---

## ğŸ§ª Testing

Tested configurations:
- 4 processes (2Ã—2 grid)
- 9 processes (3Ã—3 grid)
- 16 processes (4Ã—4 grid)
- 27 processes (3Ã—3Ã—3 cube)
- 64 processes (8Ã—8 grid, 4Ã—4Ã—4 cube)

Verification:
- **Broadcast:** All processes receive identical data âœ“
- **Gather:** Root receives data from all processes âœ“

---

## ğŸ“š References

1. MPI Standard: https://www.mpi-forum.org/
2. mpi4py Documentation: https://mpi4py.readthedocs.io/
3. Kumar et al., "Introduction to Parallel Computing"
4. Project Scope Document: `2022101099_project_scope.pdf`

---

## ğŸ“„ License

Academic project for Distributed Systems course (Semester 7), IIIT Hyderabad.

---

## ğŸ‘¥ Authors

- **Aniket Gupta** - 2022101099
- **Samarth Srikar** - 2022101106

*November 2024*
