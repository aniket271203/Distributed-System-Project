\documentclass{article}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\title{Distributed Systems Project: Mesh Topology Communication Analysis}
\author{Aniket Gupta \& Samarth Srikar}
\date{\today}

\begin{document}

\maketitle

\section{Problem Statement}
In distributed systems, efficient collective communication (Broadcast and Gather) is crucial for performance. The objective of this project is to implement and analyze these operations on 2D and 3D mesh topologies. We aim to compare the performance of different routing algorithms and topology dimensions to understand their scalability and efficiency.

\section{Approach}

\subsection{Topologies}
We implemented a generalized mesh topology system using MPI:
\begin{itemize}
    \item \textbf{2D Mesh:} Processes are arranged in a $\sqrt{P} \times \sqrt{P}$ grid.
    \item \textbf{3D Mesh:} Processes are arranged in a $\sqrt[3]{P} \times \sqrt[3]{P} \times \sqrt[3]{P}$ cube.
\end{itemize}
The 3D topology is expected to have a smaller network diameter ($3\sqrt[3]{P}$) compared to 2D ($2\sqrt{P}$), leading to lower latency.

\subsection{Algorithms}

\subsubsection{1. Dimension-Order Routing (DOR)}
This is a deterministic routing algorithm where messages are propagated along one dimension at a time.
\begin{itemize}
    \item \textbf{Broadcast:} Root $\rightarrow$ X-axis $\rightarrow$ Y-axis $\rightarrow$ Z-axis.
    \item \textbf{Gather:} Z-axis $\rightarrow$ Y-axis $\rightarrow$ X-axis $\rightarrow$ Root.
\end{itemize}
\textbf{Pros:} Simple, deadlock-free, low message redundancy.
\textbf{Cons:} Higher latency (steps) compared to optimal flooding.

\subsubsection{2. Flooding (BFS)}
We implemented a flooding algorithm based on Breadth-First Search (BFS).
\begin{itemize}
    \item Nodes send messages to all unvisited neighbors simultaneously.
    \item Propagation follows the "wavefront" or "diamond" pattern.
\end{itemize}
\textbf{Pros:} Optimal latency (steps = Manhattan distance).
\textbf{Cons:} High message complexity (redundant messages sent to shared neighbors).

\section{Implementation Details}
The project is implemented in Python using \texttt{mpi4py}.
\begin{itemize}
    \item \texttt{mesh\_topology.py}: Handles coordinate mapping and neighbor discovery.
    \item \texttt{broadcast.py} / \texttt{gather.py}: Implements DOR algorithms.
    \item \texttt{flooding.py}: Implements BFS algorithms.
    \item \texttt{visualization/}: A web-based interactive visualization to demonstrate the message flow.
\end{itemize}

\section{Verification \& Validity}
To ensure the correctness of our distributed algorithms, we implemented a rigorous verification mechanism within our ablation study script.

\subsection{Methodology}
We utilized \textbf{deterministic data generation} based on process ranks to verify both data integrity and source tracking.
\begin{itemize}
    \item \textbf{Broadcast Input:} The Root process generates an array where every element is its Rank ID (e.g., $[0.0, 0.0, \dots]$). All other processes start with empty buffers.
    \item \textbf{Gather Input:} Each process generates an array where every element is its own Rank ID (e.g., Rank $i$ generates $[i, i, \dots]$).
\end{itemize}

\subsection{Verification Logic}
\begin{enumerate}
    \item \textbf{Broadcast Verification:}
    After the operation, the Root collects the received data from all processes. It verifies that:
    \begin{itemize}
        \item Every process received a non-null array.
        \item Every element in every received array matches the Root's Rank ID.
    \end{itemize}
    
    \item \textbf{Gather Verification:}
    The Root inspects the collected data structure. It verifies that:
    \begin{itemize}
        \item The collected data contains unique data chunks corresponding to every rank in the system ($0$ to $P-1$).
        \item The total number of unique chunks matches the total number of processes.
    \end{itemize}
\end{enumerate}

All experiments presented in the following section passed these verification checks, denoted by a "PASS" status in our automated testing logs.

\section{Experiments \& Results}

\subsection{Experimental Setup}
\begin{itemize}
    \item \textbf{Platform:} Local Linux environment with OpenMPI.
    \item \textbf{Processes:} Up to 16 processes.
    \item \textbf{Metrics:} Execution Time, Communication Steps, Total Messages.
\end{itemize}

\subsection{DOR vs Flooding Comparison}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../results/dor_vs_flooding_time.png}
    \caption{Execution Time: DOR vs Flooding. Flooding shows slightly higher time in this simulation due to Python/MPI overhead for handling many messages, despite fewer steps.}
    \label{fig:time}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../results/dor_vs_flooding_msgs.png}
    \caption{Message Complexity: DOR vs Flooding. Flooding generates significantly more messages due to redundancy.}
    \label{fig:msgs}
\end{figure}

\subsection{2D vs 3D Comparison}
As predicted, the 3D topology reduces the number of communication steps compared to 2D, especially as the number of processes increases.

\subsection{Scalability Analysis (Large Data)}
We conducted experiments with varying data sizes, ranging from $10^3$ to $10^7$ floats, to analyze bandwidth limitations. The table below summarizes the results for $10^7$ floats ($\approx 80$ MB) on 8 processes.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{Mesh} & \textbf{Operation} & \textbf{Algorithm} & \textbf{Time (s)} & \textbf{Observation} \\
    \hline
    2D & Broadcast & DOR & \textbf{0.350} & Bandwidth Efficient \\
    2D & Broadcast & Flooding & 1.056 & Network Saturation \\
    \hline
    3D & Broadcast & DOR & \textbf{0.479} & Bandwidth Efficient \\
    3D & Broadcast & Flooding & 1.093 & Network Saturation \\
    \hline
    2D & Gather & DOR & 1.757 & Serialized Aggregation \\
    2D & Gather & Flooding & \textbf{1.389} & Parallel Aggregation \\
    \hline
    3D & Gather & DOR & 1.815 & Serialized Aggregation \\
    3D & Gather & Flooding & \textbf{0.895} & Parallel Aggregation \\
    \hline
    \end{tabular}
    \caption{Performance Comparison for Large Data ($10^7$ floats)}
    \label{tab:scalability}
\end{table}

\textbf{Key Findings:}
\begin{itemize}
    \item \textbf{Broadcast:} DOR is significantly faster (up to $3\times$) because it avoids redundant message transmissions. Flooding saturates the network bandwidth with duplicate packets.
    \item \textbf{Gather:} Flooding (Convergecast) outperforms DOR, especially in 3D (up to $2\times$ faster). The BFS-tree structure of Flooding utilizes the mesh's parallel links more effectively than DOR's strict axis-by-axis aggregation, which creates bottlenecks at the root.
\end{itemize}

\section{Inferences & Conclusion}
\begin{enumerate}
    \item \textbf{Topology Matters:} 3D meshes provide better latency characteristics than 2D meshes for large systems.
    \item \textbf{Algorithm Trade-off:} 
    \begin{itemize}
        \item \textbf{DOR} is bandwidth-efficient (fewer messages) and simpler to implement.
        \item \textbf{Flooding} is latency-optimal (fewer steps) but consumes more bandwidth.
    \end{itemize}
    \item \textbf{Visualization:} The interactive tool successfully demonstrates the different propagation patterns (Wavefront vs Axis-aligned).
\end{enumerate}

The project successfully implemented and analyzed the requested topologies and algorithms, confirming theoretical models through experimentation and visualization.

\end{document}
